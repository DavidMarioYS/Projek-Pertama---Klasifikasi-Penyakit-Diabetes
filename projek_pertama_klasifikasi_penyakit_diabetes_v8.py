# -*- coding: utf-8 -*-
"""Projek_Pertama_Klasifikasi_Penyakit_Diabetes v8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19f6rNOn-gYn4RRs9IdkjimWoFwG1in6a

---
# **Projek Pertama - Klasifikasi Penyakit Diabetes**


---
Author : `David Mario Yohanes Samosir`

## Intalasi Libraries
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as imbpipeline
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from google.colab import files

"""## Data Wrangling
* **Gathering Data** = Melakukan pengumpulan data serta membaca dataset
* **Assessing Data** = Memeriksa dan memahami data
* **Cleaning Data** = Membersihkan data dari kesalahan/error

### Gathering Data
- Dataset diunduh terlebih dahulu dari [Kaggle](https://www.kaggle.com/datasets/sitirahmahbasri/data-penyakit-diabetes/data).
- Upload File Dataset yang sudah di unduh
- Load Dataset
"""

# Mengunggah file dari komputer lokal
uploaded = files.upload()

# Contoh: Menyimpan file yang diunggah ke dalam file lokal di Colab
with open('diabetes.csv', 'wb') as f:
    f.write(uploaded['diabetes.csv'])

# Load Dataset
df = pd.read_csv('./diabetes.csv')
df

"""### Assessing Data"""

# Menampilkan jumlah nilai yang hilang dan nilai duplikat dalam dataset.
print("Data Kosong:", df.isnull().sum().sum())
print("Data Ganda:", df.duplicated().sum())

"""Hasil: Tidak ada nilai yang hilang dalam dataset. Namun, terdapat 1256 data duplikat yang perlu dihapus."""

df.shape

"""Hasil: Dataset sekarang memiliki 744 baris dan 9 kolom."""

# Melihat informasi umum tentang dataset, termasuk jumlah nilai non-null dan tipe data masing-masing kolom.
df.info()

"""Hasil: Semua kolom memiliki 744 nilai non-null, menunjukkan tidak ada nilai yang hilang. Tipe data mencakup int64 dan float64."""

df.describe()

"""Hasil: Statistik deskriptif memberikan gambaran umum tentang distribusi data.

Contohnya, kolom 'Glucose' memiliki nilai rata-rata 120.89 dan standar deviasi 31.94.

### Data Wrangling - Cleaning Data
"""

# Menghapus data duplikat dari dataset untuk memastikan data yang digunakan unik.
df.drop_duplicates(inplace=True)
print("Jumlah nilai duplikat:", df.duplicated().sum())

"""Hasil: Semua data duplikat berhasil dihapus, sehingga sekarang tidak ada lagi data duplikat dalam dataset.

### Exploratory Data Analysis (EDA)
"""

df.value_counts()

"""#### Explore Nilai Ssetiap Fitur Dataset"""

# Jumlah Data Pregnancies
print("Jumlah Data", df.Pregnancies.value_counts())

"""Hasil: Distribusi jumlah kehamilan bervariasi, dengan nilai paling umum adalah 1 (130 kali) dan 0 (112 kali)."""

# Jumlah Data Glucose
print("Jumlah Data", df.Glucose.value_counts())

"""Hasil: Nilai glukosa berkisar dari 0 hingga 199, dengan nilai paling umum adalah 100 (17 kali)"""

# Jumlah Data BloodPressure
print("Jumlah Data", df.BloodPressure.value_counts())

"""Hasil: Tekanan darah berkisar dari 0 hingga 122, dengan nilai paling umum adalah 70 (54 kali)."""

# Jumlah Data SkinThickness
print("Jumlah Data", df.SkinThickness.value_counts())

"""Hasil: Ketebalan kulit berkisar dari 0 hingga 110, dengan nilai paling umum adalah 0 (215 kali), menunjukkan banyak data yang hilang atau tidak terisi di kolom ini."""

# Jumlah Data Insulin
print("Jumlah Data", df.Insulin.value_counts())

"""Hasil: Nilai insulin berkisar dari 0 hingga 744, dengan nilai paling umum adalah 0 (359 kali), menunjukkan banyak data yang hilang atau tidak terisi di kolom ini."""

# Jumlah Data BMI
print("Jumlah Data", df.BMI.value_counts())

"""Hasil: Nilai BMI berkisar dari 0 hingga 80.6, dengan nilai paling umum adalah 32.0 (13 kali)."""

# Jumlah Data DiabetesPedigreeFunction
print("Jumlah Data", df.DiabetesPedigreeFunction.value_counts())

"""Hasil: Fungsi keturunan diabetes bervariasi, dengan nilai paling umum adalah 0.258 (6 kali)."""

# Jumlah Data Age
print("Jumlah Data", df.Age.value_counts())

"""Hasil: Usia berkisar dari 21 hingga 81, dengan usia paling umum adalah 22 (70 kali)."""

# Jumlah Data Outcome
print("Jumlah Data", df.Outcome.value_counts())

"""Hasil: Sebagian besar data memiliki nilai Outcome 0 (491 kali), menunjukkan ketidakseimbangan kelas yang perlu diperhatikan saat pemodelan."""

for column in df.select_dtypes(include=['float64', 'int64']).columns:
    plt.figure(figsize=(10, 6))
    sns.distplot(df[column].dropna(), kde=True)
    plt.title(f'Distribusi dari {column}')
    plt.show()

"""### Matriks Korelasi"""

# Melihat correlation matrix
correlation_matrix = df.corr()
correlation_matrix

# Visualisasi matriks korelasi
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Matriks Korelasi Fitur')
plt.show()

# Melihat korelasi dengan variabel target ('Outcome')
target_correlations = correlation_matrix['Outcome'].sort_values(ascending=False)
print("Korelasi fitur dengan variabel target (Outcome):")
print(target_correlations)

"""Hasil: Korelasi fitur dengan 'Outcome' membantu dalam menentukan fitur-fitur yang paling berpengaruh. 'Glucose', 'BMI', 'Age', dan 'Pregnancies' adalah beberapa fitur yang memiliki korelasi positif dengan 'Outcome'.

### Pengolahan Dataset (Data Preparation)
"""

# 1. Penanganan nilai yang hilang
df.fillna(df.median(), inplace=True)

# 2. Normalisasi atau standardisasi fitur numerik
numeric_features = ['Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction']
scaler = StandardScaler()
df[numeric_features] = scaler.fit_transform(df[numeric_features])

# 2. Normalisasi atau standardisasi fitur
scaler = StandardScaler()
features_to_scale = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# 3. Feature engineering
def create_features(df):
    # Kategorisasi BMI
    df['BMI_Category'] = pd.cut(df['BMI'], bins=[0, 18.5, 24.9, 29.9, np.inf], labels=['Underweight', 'Normal weight', 'Overweight', 'Obese'])

    # Fitur interaksi Glucose-Insulin
    df['Glucose_Insulin_Interaction'] = df['Glucose'] * df['Insulin']

    return df

df = create_features(df)

# Pastikan hanya kolom numerik yang digunakan untuk korelasi
numeric_df = df.select_dtypes(include=[np.number])

# 4. Seleksi fitur berdasarkan korelasi dengan variabel target
correlation_threshold = 0.2
corr_with_target = abs(numeric_df.corr()['Outcome'])
selected_features = corr_with_target[corr_with_target > correlation_threshold].index.tolist()
selected_features.remove('Outcome')  # Hapus target dari list fitur

print("Fitur yang dipilih:", selected_features)

# 5. Penanganan outlier
# Menggunakan IQR untuk mendeteksi outlier
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

df = remove_outliers(df, 'Glucose')
df = remove_outliers(df, 'BMI')
df = remove_outliers(df, 'Insulin')

# 6. Encoding fitur kategorikal
# Menggunakan one-hot encoding untuk BMI_Category
df_encoded = pd.get_dummies(df, columns=['BMI_Category'], drop_first=True)

# 7. Splitting data
X = df_encoded[selected_features]
y = df_encoded['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cek hasil
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# 8. Penanganan ketidakseimbangan kelas
# Menggunakan SMOTE untuk oversampling kelas minoritas
over_sampler = SMOTE(sampling_strategy=0.5, random_state=42)
under_sampler = RandomUnderSampler(sampling_strategy=0.7, random_state=42)
pipeline = imbpipeline(steps=[('over', over_sampler), ('under', under_sampler)])
X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)

# 9. Skalasi fitur
# Menggunakan Min-Max scaling
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)

# 10. Validasi integritas data
# Memastikan tidak ada duplikasi data
duplicates = df.duplicated()
print("Jumlah duplikasi data:", duplicates.sum())

# Cek hasil akhir
print("\nShape of X_train_scaled:", X_train_scaled.shape)
print("Shape of y_train_resampled:", y_train_resampled.shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)
print("Shape of y_test:", y_test.shape)

"""## Modeling Data
Menggunakan 3 Algoritma:
  1. Logistic Regression
  2. Decision Tree
  3. Random Forest
"""

# Inisialisasi model Logistic Regression
lr = LogisticRegression()
# Latih model
lr.fit(X_train_scaled,y_train_resampled)
# Prediksi pada data uji
y_pred_lr = lr.predict(X_test_scaled)

# Evaluating Logistic Regression
print("Logistic Regression:")
print(confusion_matrix(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_lr))

# ROC Curve for Logistic Regression
fpr, tpr, _ = roc_curve(y_test, lr.predict_proba(X_test_scaled)[:,1])
plt.plot(fpr, tpr, label='Logistic Regression (AUC = %0.2f)' % roc_auc_score(y_test, y_pred_lr))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Inisialisasi model Decision Tree
dt = DecisionTreeClassifier()
# Latih model
dt.fit(X_train_scaled,y_train_resampled)
# Prediksi pada data uji
y_pred_dt = dt.predict(X_test_scaled)

# Evaluating Decision Tree
print("Decision Tree:")
print(confusion_matrix(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_dt))

# ROC Curve for Decision Tree
fpr, tpr, _ = roc_curve(y_test, dt.predict_proba(X_test_scaled)[:,1])
plt.plot(fpr, tpr, label='Decision Tree (AUC = %0.2f)' % roc_auc_score(y_test, y_pred_dt))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# Inisialisasi model Random Forest
rf = RandomForestClassifier()
# Latih model
rf.fit(X_train_scaled,y_train_resampled)
# Prediksi pada data uji
y_pred_rf = rf.predict(X_test_scaled)

# Evaluating Random Forest
print("Random Forest:")
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("ROC-AUC Score:", roc_auc_score(y_test, y_pred_rf))

# ROC Curve for Random Forest
fpr, tpr, _ = roc_curve(y_test, rf.predict_proba(X_test_scaled)[:,1])
plt.plot(fpr, tpr, label='Random Forest (AUC = %0.2f)' % roc_auc_score(y_test, y_pred_rf))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

"""## Evaluasi Model"""

# Evaluasi Logistic Regression
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)

# Evaluasi Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

# Evaluasi Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

# Menampilkan hasil evaluasi
print(f"Logistic Regression: \nAccuracy= {accuracy_lr}, \nPrecision= {precision_lr}, \nRecall= {recall_lr}, \nF1-Score= {f1_lr}")
print(f"\n\nDecision Tree: \nAccuracy= {accuracy_dt}, \nPrecision= {precision_dt}, \nRecall= {recall_dt}, \nF1-Score= {f1_dt}")
print(f"\n\nRandom Forest: \nAccuracy= {accuracy_rf}, \nPrecision= {precision_rf}, \nRecall= {recall_rf}, \nF1-Score= {f1_rf}")

# Pilih model terbaik berdasarkan hasil evaluasi
if f1_rf > f1_dt and f1_rf > f1_lr:
    best_model = rf
elif f1_dt > f1_lr:
    best_model = dt
else:
    best_model = lr

print(f"Model terbaik: {best_model}")

# Compare results
results = pd.DataFrame({
    'Actual': y_test,
    'Logistic Regression': y_pred_lr,
    'Decision Tree': y_pred_dt,
    'Random Forest': y_pred_rf
})

# Display first 10 results for comparison
print(results)

# Data sampel untuk prediksi 1
sample_data = np.array([[6, 148, 33.6, 50]])

# Normalisasi data sampel menggunakan StandardScaler yang sudah dilatih
sample_data_scaled = scaler.transform(sample_data)

# Prediksi menggunakan Logistic Regression
pred_lr = lr.predict(sample_data_scaled)
print(f"Logistic Regression prediction: {'Diabetes' if pred_lr[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Decision Tree
pred_dt = dt.predict(sample_data_scaled)
print(f"Decision Tree prediction: {'Diabetes' if pred_dt[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Random Forest
pred_rf = rf.predict(sample_data_scaled)
print(f"Random Forest prediction: {'Diabetes' if pred_rf[0] == 1 else 'No Diabetes'}")

# Data sampel untuk prediksi 2
sample_data = np.array([[3, 210, 12.5, 13]])

# Normalisasi data sampel menggunakan StandardScaler yang sudah dilatih
sample_data_scaled = scaler.transform(sample_data)

# Prediksi menggunakan Logistic Regression
pred_lr = lr.predict(sample_data_scaled)
print(f"Logistic Regression prediction: {'Diabetes' if pred_lr[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Decision Tree
pred_dt = dt.predict(sample_data_scaled)
print(f"Decision Tree prediction: {'Diabetes' if pred_dt[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Random Forest
pred_rf = rf.predict(sample_data_scaled)
print(f"Random Forest prediction: {'Diabetes' if pred_rf[0] == 1 else 'No Diabetes'}")

# Data sampel untuk prediksi 3
sample_data = np.array([[5, 180, 35.5, 45]])

# Normalisasi data sampel menggunakan StandardScaler yang sudah dilatih
sample_data_scaled = scaler.transform(sample_data)

# Prediksi menggunakan Logistic Regression
pred_lr = lr.predict(sample_data_scaled)
print(f"Logistic Regression prediction: {'Diabetes' if pred_lr[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Decision Tree
pred_dt = dt.predict(sample_data_scaled)
print(f"Decision Tree prediction: {'Diabetes' if pred_dt[0] == 1 else 'No Diabetes'}")

# Prediksi menggunakan Random Forest
pred_rf = rf.predict(sample_data_scaled)
print(f"Random Forest prediction: {'Diabetes' if pred_rf[0] == 1 else 'No Diabetes'}")